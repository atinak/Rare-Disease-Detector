{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "399985e8-8444-4857-b79e-9d1ecae04c3d",
   "metadata": {},
   "source": [
    "#### First we prepare the dataset using orphanet_explorer package : https://github.com/atinak/orphanet_explorer\n",
    "\n",
    "data can be downloaded from https://www.orphadata.com/orphanet-scientific-knowledge-files/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a91c6ce-c89c-4465-9f02-7953b5075ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from orphanet_explorer import OrphanetDataManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf56358b-3b86-4d08-aea9-f72ca9956337",
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = OrphanetDataManager(\"output/\")\n",
    "# Define input files\n",
    "xml_files = {\n",
    "    \"references\": \"data/references.xml\",\n",
    "    \"phenotype\": \"data/en_phenotype.xml\",\n",
    "    \"consequences\": \"data/en_funct_consequences.xml\",\n",
    "    \"natural_history\": \"data/en_nat_hist_ages.xml\",\n",
    "    \"epidemiology\": \"data/en_epidimiology_prev.xml\"\n",
    "}\n",
    "\n",
    "# Process files and save merged dataset\n",
    "merged_data = manager.process_files(\n",
    "    xml_files,\n",
    "    output_file=\"merged_orphanet_data.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e35c19-163c-4060-9aa5-d622d7f455c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cd425c-3267-4afc-bf67-d4cce47d6606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03c49160-9237-4fb1-ade0-ec589f72367a",
   "metadata": {},
   "source": [
    "###### Rare Disease Prediction System: Usage Example\n",
    "##### This notebook demonstrates how to use the Rare Disease Prediction System. We'll cover:\n",
    "##### 1.  **Data Loading and Processing:**  Using `OrphanetDataProcessor` to load and preprocess the raw Orphanet data.\n",
    "##### 2.  **Feature Engineering:**  Applying the `FeatureEngineer` to transform the data into a suitable format for machine learning.\n",
    "##### 3.  **Model Training:**  Training a machine learning model using `ModelTrainer`.\n",
    "##### 4.  **Making Predictions:**  Using the `Predictor` class to make predictions on new data.\n",
    "##### 5. **API Interaction (Optional):** A brief overview of how to query your prediction service.\n",
    "##### Setup\n",
    "###### First, we need to import the necessary classes and set up the paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ba3363-98b2-4d5b-b487-03886530ef48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ad51e2-b99a-46af-8bb4-92c46f267530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Add the project root directory to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from src.data_processor import OrphanetDataProcessor\n",
    "from src.feature_engineering import FeatureEngineer\n",
    "from src.model_training import ModelTrainer\n",
    "from src.predictor import Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45b289c-b168-4412-8e78-d9f5f9a97272",
   "metadata": {},
   "source": [
    "#####  Data Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df574e3-b842-478c-bb16-105841a8eb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the data processor\n",
    "data_processor = OrphanetDataProcessor(data_dir='../data')\n",
    "\n",
    "# Load the sample data\n",
    "data_processor.load_data('merged_orphanet_data.csv')\n",
    "\n",
    "# Display the first few rows of the loaded data\n",
    "data_processor.data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32c98d8-b2e9-40ee-a38c-907e293a06dd",
   "metadata": {},
   "source": [
    "#### Now, let's parse the HPO associations, and then create the feature matrices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce0cbd6-b112-4406-af5b-ad805a181f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse HPO associations\n",
    "# hpo_df = data_processor.parse_hpo_associations()\n",
    "hpo_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b867aa3-b689-4256-a4f8-440c123fa514",
   "metadata": {},
   "source": [
    "#### Now let's parse the disability associations, average age of onset, types of inheritance and prevalence data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c02d6c-26b7-4ebe-a040-73fda12080fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse disability associations\n",
    "disability_df = data_processor.parse_disability_associations()\n",
    "disability_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d524bc81-4802-4c8b-acd0-79b6c2395062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse average age of onset\n",
    "age_of_onset_df = data_processor.parse_average_age_of_onset()\n",
    "age_of_onset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330e909e-db5c-437b-aebb-c831bc7fd83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse types of inheritance\n",
    "inheritance_df = data_processor.parse_types_of_inheritance()\n",
    "inheritance_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19557779-c541-4f31-aa1e-e955363514ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse prevalence data\n",
    "# prevalence_df = data_processor.parse_prevalence_data()\n",
    "prevalence_df.head()\n",
    "# len(prevalence_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866380ff-dd94-4a28-985e-393448fcfe25",
   "metadata": {},
   "source": [
    "#### We now have separate DataFrames for HPO associations, disability associations, age of onset, inheritance types, and prevalence.  The `prepare_data_for_ml` method in the `OrphanetDataProcessor` class will combine these into a single feature matrix (X) and target variable (y) suitable for machine learning. It handles the merging and any necessary filling of missing values.  It also includes an example of *prevalence weighting*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da18a98e-5765-4d82-8822-492eab941e06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdce1c1-e498-41df-a998-48a0952e9ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get summary information\n",
    "# summary = data_processor.get_summary_information()\n",
    "summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94105882-5421-45ff-929f-6604a6541ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create external reference features\n",
    "# ext_ref_features = data_processor.create_external_ref_features()\n",
    "# ext_ref_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf387a3-f859-4af4-8496-5d46b09ef9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HPO feature matrix\n",
    "# hpo_feature_matrix = data_processor.create_hpo_feature_matrix()\n",
    "hpo_feature_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87cc7b3-a9bd-4349-9179-70935cd63f37",
   "metadata": {},
   "source": [
    "#### Finally, let's prepare the data for machine learning.  This combines the HPO and external reference features and creates the `X` (features) and `y` (target) variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9181cd23-1cd0-4b72-80f3-d314789bc6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for machine learning\n",
    "# X, y = data_processor.prepare_data_for_ml()\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of y: {y.shape}\")\n",
    "X.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca77e9c-2d5e-485b-b015-03d5b3b328d9",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d02e914-dca0-453d-af6b-b9ebdb976543",
   "metadata": {},
   "source": [
    "#### The `create_feature_pipeline` method performs the following steps:\n",
    "##### 1.  **Identifies Feature Types:** Determines which columns are numerical and categorical.\n",
    "##### 2.  **Handles Missing Values:** Fills missing numerical values with 0 and categorical values with the most frequent value.\n",
    "##### 3.  **Scales Features:** Standardizes numerical features using `StandardScaler`.\n",
    "##### 4.  **Encodes Categorical Features:** Converts categorical features into numerical representations using `OneHotEncoder`.\n",
    "##### 5.  **Selects Features:** Selects the top *k* features using mutual information (`mutual_info_classif`) or chi-squared (`chi2`).\n",
    "##### 6.  **Reduces Dimensions (Optional):** Applies Principal Component Analysis (PCA) to reduce the dimensionality of the data.\n",
    "####     Let's save the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a8af27-81ce-4182-af56-06aa871eab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the feature engineer\n",
    "feature_engineer = FeatureEngineer(models_dir='../models')\n",
    "\n",
    "# Create the feature engineering pipeline\n",
    "X_transformed = feature_engineer.create_feature_pipeline(X, y, use_pca=True, n_features=10, n_components=5)\n",
    "\n",
    "# Display the transformed features\n",
    "X_transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d4f63f-8ed7-48e4-8e5e-bef2384bc4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the pipeline\n",
    "feature_engineer.save_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecff6a45-4a6a-426a-822c-73511aecf2d9",
   "metadata": {},
   "source": [
    "####  Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1426d10-7537-4bc1-aa2d-dfec0f2cd32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model trainer\n",
    "model_trainer = ModelTrainer(models_dir='../models')\n",
    "\n",
    "# Train a Random Forest model with Optuna optimization\n",
    "best_rf_model, best_rf_params = model_trainer.run_training_pipeline(X_transformed, y, model_selection='random_forest', use_optuna=True)\n",
    "print(f\"Best Random Forest Parameters: {best_rf_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e22635-db39-402c-a16e-eb1e37d35f44",
   "metadata": {},
   "source": [
    "#### Let's train an XGBoost Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034f3b92-39ce-4585-ae6f-3ff004ec65b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost model with Optuna optimization.\n",
    "best_xgb_model, best_xgb_params = model_trainer.run_training_pipeline(X_transformed, y, model_selection='xgboost', use_optuna=True)\n",
    "print(f\"Best XGBoost Parameters: {best_xgb_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133736bf-13c0-46b2-87a9-0ab10ccf8c7e",
   "metadata": {},
   "source": [
    "#### Making Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08298f92-096f-48cd-a787-1668ca410194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the predictor (loads the saved model and feature pipeline)\n",
    "predictor = Predictor(model_filename='best_random_forest')  # Or best_xgboost, depending on which one you want to use\n",
    "\n",
    "# Create some sample input data (this should match the structure of your original data)\n",
    "sample_input = pd.DataFrame({\n",
    "    'OrphaCode': [99999],  # Add OrphaCode. It is skipped in prepare_data_for_ml but is required here\n",
    "    'HPODisorderAssociation_df2': [\n",
    "        '[{\"HPOId\": \"HP:0000256\", \"HPOTerm\": \"Macrocephaly\", \"HPOFrequency\": \"Very frequent (99-80%)\"}, \\n',\n",
    "        ' {\"HPOId\": \"HP:0001249\", \"HPOTerm\": \"Intellectual disability\", \"HPOFrequency\": \"Frequent (79-30%)\"}]'\n",
    "    ],\n",
    "     'ExternalReferences_df1': ['{\"ICD-10\": \"Q99\"}']\n",
    "    , 'SummaryInformation_df1': ['{\"Definition\": \"This is <i>test</i> definition 1\"}']\n",
    "})\n",
    "\n",
    "# We need to preprocess the sample input using the SAME data processor\n",
    "sample_input = data_processor.data_df.append(sample_input, ignore_index = True)\n",
    "hpo_df_sample = data_processor.parse_hpo_associations()\n",
    "ext_ref_df_sample = data_processor.parse_external_references()\n",
    "hpo_feature_matrix_sample = data_processor.create_hpo_feature_matrix(hpo_df=hpo_df_sample)\n",
    "ext_ref_features_sample = data_processor.create_external_ref_features(ext_ref_df=ext_ref_df_sample)\n",
    "X_sample, _ = data_processor.prepare_data_for_ml() # We do not need the target variable\n",
    "X_sample = X_sample.tail(1) # Take only our sample\n",
    "\n",
    "# Make predictions\n",
    "predictions = predictor.predict(X_sample)\n",
    "print(f\"Predictions: {predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37d8958-42e2-4a23-aa51-88d6e257a2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get probabilities (if supported by the model)\n",
    "probabilities = predictor.predict(X_sample, return_probabilities=True)\n",
    "print(f\"Probabilities: {probabilities}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe04626-8536-477b-b30f-65f5f2ff9636",
   "metadata": {},
   "source": [
    "#### API Interaction (Optional)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84482f3a-b38a-4af8-b09a-2fcdd29ceb80",
   "metadata": {},
   "source": [
    "#### To interact with the API, you would typically send a POST request to the `/predict` endpoint with the input data in JSON format.  Here's a *conceptual* example using the `requests` library (this won't run directly in the notebook without the API server running separately):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114f71e9-bab8-4a53-9112-c6723daee602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952564d4-35e2-43db-a2f9-7aec4c64d2b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31c465b-33f5-4cff-8929-4eaf65925b13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca51976-95c9-427b-84dd-3ad9279a3b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "\n",
    "# %% [markdown]\n",
    "#\n",
    "# ```python\n",
    "# import requests\n",
    "# import json\n",
    "#\n",
    "# # API endpoint URL (replace with your actual URL if different)\n",
    "# api_url = 'http://localhost:5000/predict'\n",
    "#\n",
    "# # Sample input data (same structure as in the prediction example above)\n",
    "# input_data = {\n",
    "#    'OrphaCode': 99999,\n",
    "#     'HPODisorderAssociation_df2': [\n",
    "#         '[{\"HPOId\": \"HP:0000256\", \"HPOTerm\": \"Macrocephaly\", \"HPOFrequency\": \"Very frequent (99-80%)\"}, \\n',\n",
    "#          ' {\"HPOId\": \"HP:0001249\", \"HPOTerm\": \"Intellectual disability\", \"HPOFrequency\": \"Frequent (79-30%)\"}]'\n",
    "#     ],\n",
    "#     'ExternalReferences_df1': ['{\"ICD-10\": \"Q99\"}']\n",
    "# }\n",
    "# input_data = pd.DataFrame(input_data)\n",
    "#\n",
    "# # Send the request\n",
    "# response = requests.post(api_url, json=input_data.to_dict())\n",
    "#\n",
    "# # Check the response\n",
    "# if response.status_code == 200:\n",
    "#     result = response.json()\n",
    "#     print(f\"Prediction: {result['predictions']}\")\n",
    "# else:\n",
    "#     print(f\"Error: {response.status_code} - {response.text}\")\n",
    "# ```\n",
    "#\n",
    "# This code snippet is a *guide* for how you'd interact with the API. It shows the basic structure of sending a request and handling the response.  You'll need to adapt it to your specific needs (e.g., error handling, different input data formats).\n",
    "# To run the API:\n",
    "# 1. Navigate to the rare_disease_prediction directory in your terminal\n",
    "# 2. Run `python api/app.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b150d40-570f-4e10-ba64-0ed8a10488a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74ec87b-dccb-4ba3-a46f-ea9a307c332d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1169709-91fc-4709-9b6d-e5b2fda411c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4ebc07-bdb7-4e47-a318-a969dce04560",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
